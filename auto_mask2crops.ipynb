{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d8e1eb6f-fd73-481d-b898-cf5dffc67701",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'hydra'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mPIL\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Image\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msam2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msam2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbuild_sam\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m build_sam2\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msam2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msam2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msam2_image_predictor\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SAM2ImagePredictor\n",
      "File \u001b[1;32mc:\\Users\\Po4ka\\DRONES\\arhipelag\\sam2\\sam2\\__init__.py:7\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright (c) Meta Platforms, Inc. and affiliates.\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# All rights reserved.\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# This source code is licensed under the license found in the\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# LICENSE file in the root directory of this source tree.\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mhydra\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m initialize_config_module\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mhydra\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mglobal_hydra\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m GlobalHydra\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m GlobalHydra\u001b[38;5;241m.\u001b[39minstance()\u001b[38;5;241m.\u001b[39mis_initialized():\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'hydra'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import torch\n",
    "\n",
    "from sam2.sam2.build_sam import build_sam2\n",
    "from sam2.sam2.sam2_image_predictor import SAM2ImagePredictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8435e1ea-52dd-46dc-92c7-e267a2d0e9b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a79d50f3-8da0-4eaa-81f8-207f5522115b",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGES_DIR = \"/home/aerozrenie/second_stage/filtered/bbcrops_resized/images\"\n",
    "LABELS_BB_DIR = \"/home/aerozrenie/second_stage/filtered/bbcrops_resized/labels\"\n",
    "OUTPUT_MASKS_DIR = \"/home/aerozrenie/second_stage/filtered/bbcrops_resized/labels_masks\"\n",
    "\n",
    "os.makedirs(OUTPUT_MASKS_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac3be71-af11-4a6a-8d68-9ebb473b6f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "CHECKPOINT_PATH = \"/home/aerozrenie/second_stage/segment-anything-2/checkpoints/sam2.1_hiera_large.pt\"\n",
    "MODEL_CFG = \"configs/sam2.1/sam2.1_hiera_l.yaml\" \n",
    "\n",
    "sam2_model = build_sam2(MODEL_CFG, CHECKPOINT_PATH, device=device)\n",
    "predictor = SAM2ImagePredictor(sam2_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "330f4a5c-c942-453f-8ad4-bd7e7293fe49",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_filenames = [f for f in os.listdir(IMAGES_DIR) if f.lower().endswith(('.png', '.jpg', '.jpeg', '.JPEG', '.JPG', '.PNG'))]\n",
    "for image_filename in tqdm(image_filenames, desc=\"Обработка изображений\"):\n",
    "\n",
    "    base_filename = os.path.splitext(image_filename)[0]\n",
    "    image_path = os.path.join(IMAGES_DIR, image_filename)\n",
    "    label_bb_path = os.path.join(LABELS_BB_DIR, base_filename + \".txt\")\n",
    "    output_mask_path = os.path.join(OUTPUT_MASKS_DIR, base_filename + \".txt\")\n",
    "\n",
    "    if not os.path.exists(label_bb_path):\n",
    "        continue\n",
    "\n",
    "    image_pil = Image.open(image_path).convert(\"RGB\")\n",
    "    image_np = np.array(image_pil)\n",
    "    image_height, image_width = image_np.shape[:2]\n",
    "\n",
    "    input_boxes_with_class_ids = []\n",
    "    with open(label_bb_path, 'r') as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split()\n",
    "            if len(parts) < 5: continue\n",
    "            class_id = int(parts[0])\n",
    "            x_center_norm, y_center_norm, width_norm, height_norm = map(float, parts[1:5])\n",
    "\n",
    "            x_center_px = x_center_norm * image_width\n",
    "            y_center_px = y_center_norm * image_height\n",
    "            width_px = width_norm * image_width\n",
    "            height_px = height_norm * image_height\n",
    "\n",
    "            x_min = x_center_px - (width_px / 2)\n",
    "            y_min = y_center_px - (height_px / 2)\n",
    "            x_max = x_center_px + (width_px / 2)\n",
    "            y_max = y_center_px + (height_px / 2)\n",
    "            input_box = np.array([x_min, y_min, x_max, y_max])\n",
    "            input_boxes_with_class_ids.append({'box': input_box, 'class_id': class_id})\n",
    "\n",
    "    if not input_boxes_with_class_ids:\n",
    "        continue\n",
    "\n",
    "    all_output_lines = []\n",
    "    predictor.set_image(image_np)\n",
    "    \n",
    "    with torch.inference_mode(), torch.autocast(device.type, dtype=torch.bfloat16 if device.type == \"cuda\" else torch.float32):\n",
    "        for item in input_boxes_with_class_ids:\n",
    "            current_box = item['box']\n",
    "            current_class_id = item['class_id']\n",
    "\n",
    "            masks_pred, _, _ = predictor.predict(\n",
    "                point_coords=None,\n",
    "                point_labels=None,\n",
    "                box=current_box[None, :], \n",
    "                multimask_output=False,\n",
    "            )\n",
    "            \n",
    "            mask_tensor = masks_pred[0].squeeze() \n",
    "            mask_np_binary = (mask_tensor > 0.5).astype(np.uint8) \n",
    "\n",
    "            contours, _ = cv2.findContours(mask_np_binary, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "            for contour in contours:\n",
    "                if contour.shape[0] < 3: continue\n",
    "\n",
    "                normalized_contour = contour.astype(np.float32)\n",
    "                normalized_contour[:, 0, 0] /= image_width  \n",
    "                normalized_contour[:, 0, 1] /= image_height \n",
    "                \n",
    "                normalized_contour = np.clip(normalized_contour, 0.0, 1.0)\n",
    "\n",
    "                segment_points = normalized_contour.squeeze().reshape(-1).tolist()\n",
    "                \n",
    "                if len(segment_points) >= 6 and len(segment_points) % 2 == 0:\n",
    "                    line_out = f\"{current_class_id} \" + \" \".join(map(str, segment_points))\n",
    "                    all_output_lines.append(line_out)\n",
    "\n",
    "    if all_output_lines:\n",
    "        with open(output_mask_path, 'w') as f_out:\n",
    "            for line in all_output_lines:\n",
    "                f_out.write(line + \"\\n\")\n",
    "    else:\n",
    "        print(f\"Не найдено валидных сегментов для {image_filename}\")\n",
    "\n",
    "print(\"Обработка завершена.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c1fd1c-dd72-48ed-9a78-2eacd7ac5c46",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ef3799-d8b0-4851-87c0-c081d760bb6a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
